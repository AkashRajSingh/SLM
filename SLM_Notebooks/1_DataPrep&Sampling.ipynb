{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP119bhIBd/o90xyw+ajbx8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Small Language Model (SLM) - Data Preparation & Sampling\n",
        "This notebook covers loading and preprocessing text data, tokenization using GPT-2 BPE, and creating a PyTorch dataset with sliding-window sampling to generate input-target sequences for training a small language model.\n"
      ],
      "metadata": {
        "id": "kBkdhMIKoCMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Tokenizer Class (For Intution)"
      ],
      "metadata": {
        "id": "3lqWiZTCEc2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ptdpvdbwUzuO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset (tiny text corpus for demo)\n",
        "sample_corpus = [\n",
        "    \"Hello, World. I am Akash!\",\n",
        "    \"I am learning to tokenize text.\",\n",
        "    \"Hello again, World.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "g5HoNMHPebZ3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex-based tokenization\n",
        "def regex_tokenizer(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize a text string into words and punctuation marks,\n",
        "    while removing whitespace tokens.\n",
        "\n",
        "    Example:\n",
        "    Input:  \"Hello, World. I am Akash!\"\n",
        "    Output: ['Hello', ',', 'World', '.', 'I', 'am', 'Akash', '!']\n",
        "    \"\"\"\n",
        "    # \\w+ matches words (letters, digits, underscore)\n",
        "    # [^\\w\\s] matches any punctuation (not word, not space)\n",
        "    # This ensures words and punctuation are separate tokens\n",
        "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
        "    return tokens\n",
        "\n",
        "# Test tokenizer\n",
        "for sentence in sample_corpus:\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Tokens: {regex_tokenizer(sentence)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUxAc5tFem5t",
        "outputId": "ca148b4a-c7a1-4676-8b8d-1984bc88978e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hello, World. I am Akash!\n",
            "Tokens: ['Hello', ',', 'World', '.', 'I', 'am', 'Akash', '!']\n",
            "\n",
            "Input: I am learning to tokenize text.\n",
            "Tokens: ['I', 'am', 'learning', 'to', 'tokenize', 'text', '.']\n",
            "\n",
            "Input: Hello again, World.\n",
            "Tokens: ['Hello', 'again', ',', 'World', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tokenizer class\n",
        "class SimpleTokenizer:\n",
        "    \"\"\"\n",
        "    A basic word-level tokenizer:\n",
        "    - Builds vocabulary from a corpus\n",
        "    - Encodes text to list of token IDs\n",
        "    - Decodes IDs back to text\n",
        "    - Handles unknown tokens with <|unk|>\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # special tokens\n",
        "        self.unk_token = \"<|unk|>\"\n",
        "        self.pad_token = \"<|pad|>\"\n",
        "\n",
        "        # token mappings\n",
        "        self.token2id: Dict[str, int] = {}\n",
        "        self.id2token: Dict[int, str] = {}\n",
        "\n",
        "    def build_vocab(self, corpus: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Build vocabulary from a list of text strings.\n",
        "        Uses regex tokenizer to split words/punctuations.\n",
        "        \"\"\"\n",
        "        vocab = set()\n",
        "        for text in corpus:\n",
        "            tokens = regex_tokenizer(text)\n",
        "            vocab.update(tokens)\n",
        "\n",
        "        # Add special tokens first\n",
        "        vocab = [self.pad_token, self.unk_token] + sorted(vocab)\n",
        "\n",
        "        # Create mappings\n",
        "        self.token2id = {tok: idx for idx, tok in enumerate(vocab)}\n",
        "        self.id2token = {idx: tok for tok, idx in self.token2id.items()}\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"\n",
        "        Convert text into list of token IDs.\n",
        "        Unknown tokens are mapped to <|unk|>.\n",
        "        \"\"\"\n",
        "        tokens = regex_tokenizer(text)\n",
        "        return [self.token2id.get(tok, self.token2id[self.unk_token]) for tok in tokens]\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        \"\"\"\n",
        "        Convert list of IDs back to text string.\n",
        "        Joins tokens with space, but you may customize\n",
        "        spacing rules depending on your use case.\n",
        "        \"\"\"\n",
        "        tokens = [self.id2token.get(i, self.unk_token) for i in ids]\n",
        "        return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "qd-kPNjiqQDi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize tokenizer and build vocab\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.build_vocab(sample_corpus)\n",
        "\n",
        "print(\"Vocabulary size:\", len(tokenizer.token2id))\n",
        "print(\"Sample of vocab items:\", list(tokenizer.token2id.items())[:15], \"\\n\")\n",
        "\n",
        "# Encode text\n",
        "sentence = \"Hello, I am learning NLP.\"\n",
        "encoded = tokenizer.encode(sentence)\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"Encoded IDs:\", encoded)\n",
        "\n",
        "# Decode back\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(\"Decoded back:\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Emaagrqv34",
        "outputId": "bc65686e-da51-47c2-fe68-0d6e5dd11e74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 16\n",
            "Sample of vocab items: [('<|pad|>', 0), ('<|unk|>', 1), ('!', 2), (',', 3), ('.', 4), ('ChatGPT', 5), ('Hello', 6), ('I', 7), ('World', 8), ('again', 9), ('am', 10), ('is', 11), ('learning', 12), ('text', 13), ('to', 14)] \n",
            "\n",
            "Original sentence: Hello, I am learning NLP.\n",
            "Encoded IDs: [6, 3, 7, 10, 12, 1, 4]\n",
            "Decoded back: Hello , I am learning <|unk|> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tiktoken - The Byte Pair Encoding Tokenizer"
      ],
      "metadata": {
        "id": "O9cIJvJEoLkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "%pip install -q torch tiktoken datasets KaggleHub\n"
      ],
      "metadata": {
        "id": "EFgFljx5q0nU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "import kagglehub\n"
      ],
      "metadata": {
        "id": "MthmmspPKqQD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download dataset from GitHub\n",
        "# \"The Verdict\" dataset\n",
        "import requests\n",
        "\n",
        "# Download the text file\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Save locally (optional)\n",
        "with open(\"the-verdict.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "# Check character and word stats\n",
        "print(f\"Number of characters: {len(text)}\")\n",
        "print(f\"Number of words: {len(text.split())}\")\n",
        "print(f\"First 300 characters preview:\\n{text[:300]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MguZ_9IsRPo5",
        "outputId": "791e13aa-1cfe-4eb7-eb18-8f694c17c781"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters: 20479\n",
            "Number of words: 3634\n",
            "First 300 characters preview:\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(f\"Number of tokens: {len(tokenizer.encode(text))}\")\n",
        "\n",
        "# If you'd like to experiment with \"allowed_special\", here's an example\n",
        "# enc_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5DMCXhRKu6u",
        "outputId": "4d880f92-8fd0-468a-8382-29d2120e8ed5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and Dataloader"
      ],
      "metadata": {
        "id": "GStw2SSoofFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SLMDataset with sliding window\n",
        "\n",
        "class SLMDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Small Language Model dataset with sliding window tokenization.\n",
        "\n",
        "    Args:\n",
        "        text (str): full raw text.\n",
        "        tokenizer (tiktoken.Encoding): tokenizer object.\n",
        "        max_length (int): sequence length (including input & target).\n",
        "        stride (int): overlap size between chunks.\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str, tokenizer, max_length: int, stride: int):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Encode full text into token IDs\n",
        "        token_ids = tokenizer.encode(text)\n",
        "\n",
        "        # Sliding window chunks\n",
        "        start = 0\n",
        "        while start + max_length < len(token_ids):\n",
        "            chunk = token_ids[start:start + max_length]\n",
        "\n",
        "            # Input is everything except the last token\n",
        "            inp = chunk[:-1]\n",
        "            # Target is everything except the first token (shifted by 1)\n",
        "            tgt = chunk[1:]\n",
        "\n",
        "            self.input_ids.append(torch.tensor(inp, dtype=torch.long))\n",
        "            self.target_ids.append(torch.tensor(tgt, dtype=torch.long))\n",
        "\n",
        "            start += stride\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"target_ids\": self.target_ids[idx]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "MLESr1kzPS0X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader creation\n",
        "def create_dataloader(\n",
        "    text: str,\n",
        "    tokenizer,\n",
        "    batch_size: int = 4,\n",
        "    max_length: int = 256,\n",
        "    stride: int = 128,\n",
        "    shuffle: bool = True,\n",
        "    drop_last: bool = True,\n",
        "    num_workers: int = 0\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a PyTorch DataLoader from raw text with sliding window sampling.\n",
        "    \"\"\"\n",
        "    dataset = SLMDataset(text, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "# Example usage\n",
        "dataloader = create_dataloader(raw_text, tokenizer, batch_size=4, max_length=128, stride=64)\n",
        "\n",
        "for batch in dataloader:\n",
        "    print(\"Batch input shape:\", batch[\"input_ids\"].shape)\n",
        "    print(\"Batch target shape:\", batch[\"target_ids\"].shape)\n",
        "    print(\"Example input IDs:\", batch[\"input_ids\"][0][:20])\n",
        "    print(\"Example target IDs:\", batch[\"target_ids\"][0][:20])\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdpj5pvuRffQ",
        "outputId": "93fefce7-a549-49f1-8ebb-4a70feb3bbe3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch input shape: torch.Size([4, 127])\n",
            "Batch target shape: torch.Size([4, 127])\n",
            "Example input IDs: tensor([  339,  3947,   284,   307,  1592,  2259,   739,   438, 14363,   898,\n",
            "         9408,   355,   281,  2134,   329,  5482,  4447,   290,   753,  1072])\n",
            "Example target IDs: tensor([ 3947,   284,   307,  1592,  2259,   739,   438, 14363,   898,  9408,\n",
            "          355,   281,  2134,   329,  5482,  4447,   290,   753,  1072,    13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization utility:  Intuitive\n",
        "def visualize_sequence_pairs(dataset, tokenizer, idx=0, max_steps=10):\n",
        "    \"\"\"\n",
        "    Show input sequence -> next target (both tokens and IDs).\n",
        "\n",
        "    Args:\n",
        "        dataset: SLMDataset object\n",
        "        tokenizer: tiktoken Encoding\n",
        "        idx: which dataset example to visualize\n",
        "        max_steps: number of steps to show\n",
        "    \"\"\"\n",
        "    example = dataset[idx]\n",
        "    input_ids = example[\"input_ids\"]\n",
        "    target_ids = example[\"target_ids\"]\n",
        "\n",
        "    print(\"          Token view \\n\")\n",
        "    print(\"Input Tokens --> Target Tokens\")\n",
        "    for step in range(min(max_steps, len(input_ids))):\n",
        "        inp_tokens = tokenizer.decode(input_ids[: step + 1].tolist())\n",
        "        tgt_token = tokenizer.decode([target_ids[step].item()])\n",
        "        print(f\"{inp_tokens} --> {tgt_token}\")\n",
        "\n",
        "    print(\"\\n        ID view \\n\")\n",
        "    print(\"Input Token IDs --> Target Token IDs\")\n",
        "    for step in range(min(max_steps, len(input_ids))):\n",
        "        inp_ids = input_ids[: step + 1].tolist()\n",
        "        tgt_id = target_ids[step].item()\n",
        "        print(f\"{inp_ids} --> {tgt_id}\")\n",
        "\n",
        "# Example usage\n",
        "dataset = SLMDataset(raw_text, tokenizer, max_length=32, stride=16)\n",
        "print(\"    Visualization:  Intuitive \\n\")\n",
        "visualize_sequence_pairs(dataset, tokenizer, idx=0, max_steps=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLXH61arRlY-",
        "outputId": "5aca54e7-2854-409c-8105-a04becebc204"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Visualization:  Intuitive \n",
            "\n",
            "          Token view \n",
            "\n",
            "Input Tokens --> Target Tokens\n",
            "I -->  H\n",
            "I H --> AD\n",
            "I HAD -->  always\n",
            "I HAD always -->  thought\n",
            "I HAD always thought -->  Jack\n",
            "I HAD always thought Jack -->  G\n",
            "I HAD always thought Jack G --> is\n",
            "I HAD always thought Jack Gis --> burn\n",
            "\n",
            "        ID view \n",
            "\n",
            "Input Token IDs --> Target Token IDs\n",
            "[40] --> 367\n",
            "[40, 367] --> 2885\n",
            "[40, 367, 2885] --> 1464\n",
            "[40, 367, 2885, 1464] --> 1807\n",
            "[40, 367, 2885, 1464, 1807] --> 3619\n",
            "[40, 367, 2885, 1464, 1807, 3619] --> 402\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402] --> 271\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271] --> 10899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch tensors that come out of the dataset\n",
        "\n",
        "def show_batch_tensors(dataloader, tokenizer, n_batches=1):\n",
        "    \"\"\"\n",
        "    Show raw tensors (input_ids and target_ids) from a dataloader,\n",
        "    and decode them into a matrix of text tokens.\n",
        "\n",
        "    Args:\n",
        "        dataloader: PyTorch DataLoader from create_dataloader\n",
        "        tokenizer: tiktoken Encoding\n",
        "        n_batches: how many batches to display\n",
        "    \"\"\"\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        if i >= n_batches:\n",
        "            break\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        target_ids = batch[\"target_ids\"]\n",
        "\n",
        "        print(f\"                 === Batch {i} ===\")\n",
        "        print(\"Input tensor shape :\", input_ids.shape)\n",
        "        print(\"Target tensor shape:\", target_ids.shape)\n",
        "        print(\"\\nInput IDs:\\n\", input_ids)\n",
        "        print(\"\\nTarget IDs:\\n\", target_ids)\n",
        "\n",
        "        # Decode each row to text\n",
        "       #print(\"\\n=== Decoded input sequences ===\")\n",
        "       #for row in input_ids:\n",
        "       #    print(tokenizer.decode(row.tolist()))\n",
        "\n",
        "       #print(\"\\n=== Decoded target sequences ===\")\n",
        "       #for row in target_ids:\n",
        "       #    print(tokenizer.decode(row.tolist()))\n",
        "\n",
        "        #print(\"=\" * 80)\n",
        "\n",
        "\n",
        "print(\"       Visualization:  Actual \\n\")\n",
        "show_batch_tensors(dataloader, tokenizer, n_batches=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EyJcivHTLKM",
        "outputId": "192a3a6d-c2fe-4b14-b647-655276b9e94d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Visualization:  Actual \n",
            "\n",
            "                 === Batch 0 ===\n",
            "Input tensor shape : torch.Size([4, 127])\n",
            "Target tensor shape: torch.Size([4, 127])\n",
            "\n",
            "Input IDs:\n",
            " tensor([[  550,   587, 11191,   416,  3499,  1466,    25,   484,   550, 26546,\n",
            "          1068,   465,  1242,    11,   340,   550,   587,   302,  1144,   287,\n",
            "           262,  3024,    12,  4803,   286,   511,   512,  1741,    13,   843,\n",
            "           340,   373,  4361,  5048,   425,   284,  3465,   644,  1245,   262,\n",
            "           366, 25124,  3101,  8137,   286, 16957,  1696,   414,     1,   357,\n",
            "            40,  9577,  4544,  9325,   701,     8,   373,  1719,   319,   683,\n",
            "            13,   198,   198,    40,   423,  4750,   326,  9074,    13,   402,\n",
            "           271, 10899,   373,  5527,    26,   290,   340,   373,  3393, 34953,\n",
            "           856,   326,   607,  5229,   373, 37895,   422,   428, 25179,   257,\n",
            "         19217,   475,  8904, 14676,    13,   632,   318,    11,   355,   257,\n",
            "          3896,    11,   262,   661,   508, 40987,  1637,   508,   651,   749,\n",
            "           503,   286,   340,    26,   290,  3619,   338, 19992, 31564,   286,\n",
            "           465,  3656,   338,  1263,  5236,  9343,   683],\n",
            "        [  673,   373, 10032,   286,   852, 13055,   366, 34751,   306,     1,\n",
            "           438,   392,  1865,   407,   284,  4425,   281, 22037,   286,   262,\n",
            "         32073,    13,   198,   198,     1,  1026,   338,   262,   938,   339,\n",
            "         13055,    11,   345,   760,   553,  9074,    13,   402,   271, 10899,\n",
            "           531,   351, 27322,   540, 11293,    13,   366,   464,   938,   475,\n",
            "           530,   553,   673, 19267,  5223,   438,     1,  4360,   262,   584,\n",
            "          1595,   470,   954,    11,   780,   339,  6572,   340,   526,   198,\n",
            "           198,     1, 49174,   276,   340,  1701,   314,   373,   546,   284,\n",
            "          1061,   510,   428, 18437,   618,   314,  2982,   257,  2366,  9662,\n",
            "           290,  2497,  3619,  2241,   319,   262, 11387,    13,   198,   198,\n",
            "          1722,   339,  6204,   612,    11,   465,  2832,   287,   262, 16511,\n",
            "           286,   465, 11555,   303,  7821, 13209,    11,   262,  7888,  7586,\n",
            "          9813,   286,  4190,  7121,   736,   422,   465],\n",
            "        [  286,   616,  4286,   705,  1014,   510,    26,   475,   314,   836,\n",
            "           470,   892,   286,   326,    11,  1770,    13,  8759,  2763,   438,\n",
            "          1169,  2994,   284,   943, 17034,   318,   477,   314,   892,   286,\n",
            "           526,   383,  1573,    11,   319,  9074,    13,   536,  5469,   338,\n",
            "         11914,    11, 33096,   663,  4808,  3808,    62,   355,   996,   484,\n",
            "           547, 12548,   287,   281, 13079,   410, 12523,   286, 22353,    13,\n",
            "           843,   340,   373,   407,   691,   262,  9074,    13,   536, 48819,\n",
            "           508, 25722,   276,    13, 11161,   407,   262, 40123, 18113,   544,\n",
            "          9325,   701,    11,   379,   262,   938,   402,  1617,   261, 12917,\n",
            "           905,    11,  5025,   502,   878,   402,   271, 10899,   338,   366,\n",
            "         31640,    12,    67, 20811,     1,   284,   910,    11,   351, 10953,\n",
            "           287,   607,  2951,    25,   366,  1135,  2236,   407,   804,  2402,\n",
            "           663,   588,   757, 13984,   198,   198,  5779],\n",
            "        [  503,  4291,   262,  4252, 18250,  8812,   558,    13,   198,   198,\n",
            "            40, 27846,   706,   683,    11,  7425,   416,   465,   938,  1573,\n",
            "            13, 12622, 41379,   293,   373,    11,   287,  1109,    11,  5033,\n",
            "           262,   582,   286,   262,  2589,   438,   292,  3619,  2241,    11,\n",
            "           530,  1244,  1234,   340,    11,   550,   587,   262,   582,   286,\n",
            "           262,  1711,    13,   383,  7099,  6802,   373,   531,   284,   423,\n",
            "          7042,  2241,   379,   616,  1545,   338,  3625,    11,   290,   314,\n",
            "         14028,   611,   257,   256, 11912,   286, 35394,   739, 10724,   262,\n",
            "          6846,   338, 11428,   450,    67,  3299,    13,   887,   645,   438,\n",
            "          1640,   340,   373,   407, 10597,   706,   326,  1785,   326,   262,\n",
            "          4808, 13698, 10322,  6532,    62,  8263,    12,  9649,   550,  9258,\n",
            "           284,  3359,   511,   366,  8642,   521,   829,   526,   198,   198,\n",
            "            40,  2900,   284,  9074,    13,   402,   271]])\n",
            "\n",
            "Target IDs:\n",
            " tensor([[  587, 11191,   416,  3499,  1466,    25,   484,   550, 26546,  1068,\n",
            "           465,  1242,    11,   340,   550,   587,   302,  1144,   287,   262,\n",
            "          3024,    12,  4803,   286,   511,   512,  1741,    13,   843,   340,\n",
            "           373,  4361,  5048,   425,   284,  3465,   644,  1245,   262,   366,\n",
            "         25124,  3101,  8137,   286, 16957,  1696,   414,     1,   357,    40,\n",
            "          9577,  4544,  9325,   701,     8,   373,  1719,   319,   683,    13,\n",
            "           198,   198,    40,   423,  4750,   326,  9074,    13,   402,   271,\n",
            "         10899,   373,  5527,    26,   290,   340,   373,  3393, 34953,   856,\n",
            "           326,   607,  5229,   373, 37895,   422,   428, 25179,   257, 19217,\n",
            "           475,  8904, 14676,    13,   632,   318,    11,   355,   257,  3896,\n",
            "            11,   262,   661,   508, 40987,  1637,   508,   651,   749,   503,\n",
            "           286,   340,    26,   290,  3619,   338, 19992, 31564,   286,   465,\n",
            "          3656,   338,  1263,  5236,  9343,   683,    11],\n",
            "        [  373, 10032,   286,   852, 13055,   366, 34751,   306,     1,   438,\n",
            "           392,  1865,   407,   284,  4425,   281, 22037,   286,   262, 32073,\n",
            "            13,   198,   198,     1,  1026,   338,   262,   938,   339, 13055,\n",
            "            11,   345,   760,   553,  9074,    13,   402,   271, 10899,   531,\n",
            "           351, 27322,   540, 11293,    13,   366,   464,   938,   475,   530,\n",
            "           553,   673, 19267,  5223,   438,     1,  4360,   262,   584,  1595,\n",
            "           470,   954,    11,   780,   339,  6572,   340,   526,   198,   198,\n",
            "             1, 49174,   276,   340,  1701,   314,   373,   546,   284,  1061,\n",
            "           510,   428, 18437,   618,   314,  2982,   257,  2366,  9662,   290,\n",
            "          2497,  3619,  2241,   319,   262, 11387,    13,   198,   198,  1722,\n",
            "           339,  6204,   612,    11,   465,  2832,   287,   262, 16511,   286,\n",
            "           465, 11555,   303,  7821, 13209,    11,   262,  7888,  7586,  9813,\n",
            "           286,  4190,  7121,   736,   422,   465,  2330],\n",
            "        [  616,  4286,   705,  1014,   510,    26,   475,   314,   836,   470,\n",
            "           892,   286,   326,    11,  1770,    13,  8759,  2763,   438,  1169,\n",
            "          2994,   284,   943, 17034,   318,   477,   314,   892,   286,   526,\n",
            "           383,  1573,    11,   319,  9074,    13,   536,  5469,   338, 11914,\n",
            "            11, 33096,   663,  4808,  3808,    62,   355,   996,   484,   547,\n",
            "         12548,   287,   281, 13079,   410, 12523,   286, 22353,    13,   843,\n",
            "           340,   373,   407,   691,   262,  9074,    13,   536, 48819,   508,\n",
            "         25722,   276,    13, 11161,   407,   262, 40123, 18113,   544,  9325,\n",
            "           701,    11,   379,   262,   938,   402,  1617,   261, 12917,   905,\n",
            "            11,  5025,   502,   878,   402,   271, 10899,   338,   366, 31640,\n",
            "            12,    67, 20811,     1,   284,   910,    11,   351, 10953,   287,\n",
            "           607,  2951,    25,   366,  1135,  2236,   407,   804,  2402,   663,\n",
            "           588,   757, 13984,   198,   198,  5779, 28112],\n",
            "        [ 4291,   262,  4252, 18250,  8812,   558,    13,   198,   198,    40,\n",
            "         27846,   706,   683,    11,  7425,   416,   465,   938,  1573,    13,\n",
            "         12622, 41379,   293,   373,    11,   287,  1109,    11,  5033,   262,\n",
            "           582,   286,   262,  2589,   438,   292,  3619,  2241,    11,   530,\n",
            "          1244,  1234,   340,    11,   550,   587,   262,   582,   286,   262,\n",
            "          1711,    13,   383,  7099,  6802,   373,   531,   284,   423,  7042,\n",
            "          2241,   379,   616,  1545,   338,  3625,    11,   290,   314, 14028,\n",
            "           611,   257,   256, 11912,   286, 35394,   739, 10724,   262,  6846,\n",
            "           338, 11428,   450,    67,  3299,    13,   887,   645,   438,  1640,\n",
            "           340,   373,   407, 10597,   706,   326,  1785,   326,   262,  4808,\n",
            "         13698, 10322,  6532,    62,  8263,    12,  9649,   550,  9258,   284,\n",
            "          3359,   511,   366,  8642,   521,   829,   526,   198,   198,    40,\n",
            "          2900,   284,  9074,    13,   402,   271, 10899]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Embeddings [Token Embeddings + Positional Embeddings]"
      ],
      "metadata": {
        "id": "3NUESguSj5Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize word embeddings for a sentence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "Toy_sentence= [\"I\",\"live\",\"in\",\"India\"]\n",
        "token_ids = torch.tensor([2, 5, 7, 9])  # token IDs for each word\n",
        "vocab_size = 10\n",
        "embed_dim = 8\n",
        "\n",
        "# Word embedding layer\n",
        "word_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "\n",
        "# Get embeddings for the sentence\n",
        "word_embeds = word_embedding(token_ids)\n",
        "print(\"Word embeddings shape:\", word_embeds.shape)  # (sequence_length, embed_dim)\n",
        "for i, embed in enumerate(word_embeds):\n",
        "    print(f\"{Toy_sentence[i]} => \\n token ID: {token_ids[i]} \\n Embedding: {embed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EyzHOECUjHd",
        "outputId": "2c178f6e-4107-4875-fd16-e2cee6f6482e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embeddings shape: torch.Size([4, 8])\n",
            "I => \n",
            " token ID: 2 \n",
            " Embedding: tensor([ 2.1047,  0.1839, -0.3299, -0.5873,  0.1215, -0.9598,  0.7810, -1.3862],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "live => \n",
            " token ID: 5 \n",
            " Embedding: tensor([-1.0277,  0.5010,  1.1364, -0.2753, -0.5766, -2.0788,  0.4422,  0.5329],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "in => \n",
            " token ID: 7 \n",
            " Embedding: tensor([-0.3570,  0.1095, -1.2302,  0.6774, -0.6796, -0.1230,  1.0147, -0.5500],\n",
            "       grad_fn=<UnbindBackward0>)\n",
            "India => \n",
            " token ID: 9 \n",
            " Embedding: tensor([ 0.9634, -0.5005, -0.6265, -1.0725,  0.7942, -0.0557, -0.9855, -0.9396],\n",
            "       grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize positional embeddings (absolute positional embeddings)\n",
        "\n",
        "seq_length = len(token_ids)\n",
        "\n",
        "# Positional embedding layer\n",
        "pos_embedding = nn.Embedding(num_embeddings=seq_length, embedding_dim=embed_dim)\n",
        "\n",
        "# Get positional embeddings for each position\n",
        "positions = torch.arange(seq_length)        # [0, 1, 2, 3]\n",
        "pos_embeds = pos_embedding(positions)\n",
        "\n",
        "print(\"Positional embeddings shape:\", pos_embeds.shape)  # (sequence_length, embed_dim)\n",
        "\n",
        "\n",
        "\n",
        "# Combine word + positional embeddings\n",
        "\n",
        "input_embeds = word_embeds + pos_embeds\n",
        "print(\"\\n\\n Input embeddings shape:\", input_embeds.shape)  # (sequence_length, embed_dim)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiUhsAMZkYT8",
        "outputId": "484fb686-c90b-4cd5-e608-6d04897fc274"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positional embeddings shape: torch.Size([4, 8])\n",
            "\n",
            "\n",
            " Input embeddings shape: torch.Size([4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Embedding Layer\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 50257       # GPT-2 tokenizer vocab size\n",
        "output_dim = 256         # embedding dimension\n",
        "batch_size = 8\n",
        "max_length = 4           # sequence length\n",
        "\n",
        "# Word embeddings for the batch\n",
        "word_embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=output_dim)\n",
        "batch_token_ids = torch.randint(0, vocab_size, (batch_size, max_length))\n",
        "word_embeds = word_embedding_layer(batch_token_ids)  # shape: (8, 4, 256)\n",
        "\n",
        "# Absolute positional embeddings\n",
        "context_length = max_length\n",
        "pos_embedding_layer = nn.Embedding(num_embeddings=context_length, embedding_dim=output_dim)\n",
        "\n",
        "# torch.arange(context_length) gives: [0, 1, 2, 3]\n",
        "pos_indices = torch.arange(context_length)\n",
        "pos_embeds = pos_embedding_layer(pos_indices)        # shape: (4, 256)\n",
        "\n",
        "# Expand positional embeddings for batch\n",
        "pos_embeds = pos_embeds.unsqueeze(0).expand(batch_size, -1, -1)  # shape: (8, 4, 256)\n",
        "\n",
        "# Final input embeddings\n",
        "input_embeddings = word_embeds + pos_embeds\n",
        "print(\"Input embeddings shape:\", input_embeddings.shape)  # (8, 4, 256)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8O7DjPamnFg",
        "outputId": "0639df81-36cc-4d2d-d182-7da28cf1ed78"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embeddings shape: torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVu35fTJnh0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}